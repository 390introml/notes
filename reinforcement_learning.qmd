:::{.callout-caution}
This chapter's HTML notes are in active development. For a more stable version, you can access the PDF notes [here](https://introml.mit.edu/_static/spring25/notes.pdf).
:::

# Reinforcement Learning

*Reinforcement learning* (RL) is a type of machine learning where an agent learns to make decisions by interacting with an environment. Unlike other learning paradigms, RL has several distinctive characteristics:

- The agent interacts directly with an environment, receiving feedback in the form of rewards or penalties
- The agent has agency over what information it seeks to gain from the environment
- The agent updates its decision-making strategy incrementally as it gains more experience

In a reinforcement learning problem, the interaction between the agent and environment follows a specific pattern:

::: {.imagify}
  \begin{tikzpicture}[main node/.style={draw,rounded corners,font=\Large},
      arr/.style={->, thick, shorten <=5pt, shorten >=5pt,}]
    \node[main node] (rl) at (0,1) {Learner};
    \node[main node] (env) at (0,-1) {Environment};
    \draw (env.west) edge[arr, bend left=50, looseness=1.1]
      node[right] {reward} ($ (rl.west) + (0, -0.1)$);
    \draw ($ (env.west) + (0,-0.1)$) edge[arr, bend left=70, looseness=1.5]
      node[left] {state} ($ (rl.west) + (0, 0.1)$);
    \draw (rl.east) edge[arr, bend left=70, looseness=1.5]
      node[right] {action} (env.east);
  \end{tikzpicture}
:::


The interaction cycle proceeds as follows:

1. Agent observes the current *state* $s^{(i)}$
2. Agent selects and executes an *action* $a^{(i)}$
3. Agent receives a *reward* $r^{(i)}$ from the environment
4. Agent observes the new *state* $s^{(i+1)}$
5. Agent selects and executes a new *action* $a^{(i+1)}$
6. Agent receives a new *reward* $r^{(i+1)}$
7. This cycle continues...

Similar to MDP @sec-mdps, in an RL problem, the agent's goal is to learn a *policy* - a mapping from states to actions - that maximizes its expected cumulative reward over time. This policy guides the agent's decision-making process, helping it choose actions that lead to the most favorable outcomes.



## Reinforcement learning algorithms overview {#rl}

A *reinforcement learning (RL) algorithm* is a kind of a policy that depends on the whole history of states, actions, and rewards and selects the next action to take. There are several different ways to measure the quality of an RL algorithm, including:

- Ignoring the $r^{(i)}$ values that it gets *while* learning, but considering how many interactions with the environment are required for it to learn a policy $\pi: \mathcal{S} \rightarrow \mathcal{A}$ that is nearly optimal.

- Maximizing the expected sum of discounted rewards while it is learning.

Most of the focus is on the first criterion (which is called “sample efficiency”), because the second one is very difficult. The first criterion is reasonable when the learning can take place somewhere safe (imagine a robot learning, inside the robot factory, where it can’t hurt itself too badly) or in a simulated environment.

Approaches to reinforcement learning differ significantly according to what kind of hypothesis or model is being learned. Roughly speaking, RL methods can be categorized into model-free methods and model-based methods. The main distinction is that model-based methods explicitly learn the transition and reward models to assist the end-goal of learning a policy; model-free methods do not. We will start our discussion with the model-free methods, and introduce two of the arguably most popular types of algorithms, Q-learning @sec-q_learning and policy gradient @sec-rl_policy_search. We then describe model-based methods @sec-rl_model_based. Finally, we briefly consider "bandit" problems @sec-bandit, which differ from our MDP learning context by having probabilistic rewards.

### Model-free methods

Model-free methods are methods that do not explicitly learn transition and reward models. Depending on what is explicitly being learned, model-free methods are sometimes further categorized into value-based methods (where the goal is to learn/estimate a value function) and policy-based methods (where the goal is to directly learn an optimal policy). It’s important to note that such categorization is approximate and the boundaries are blurry. In fact, current RL research tends to combine the learning of value functions, policies, and transition and reward models all into a complex learning algorithm, in an attempt to combine the strengths of each approach.

### Q-learning {#sec-q_learning}

Q-learning is a frequently used class of RL algorithms that concentrates on learning (estimating) the state-action value function, i.e., the $Q$ function. Specifically, recall the MDP value-iteration update:



$$
\begin{equation}
  Q(s,a) = R(s,a) + \gamma \sum_{s'} T(s,a,s')\max_{a'}Q(s',a')
\end{equation}
$$

::: {.column-margin}
The thing that most students seem to get confused about is when we do value iteration and when we do Q-learning. Value iteration assumes you know $T$ and $R$ and just need to _compute_ $Q$. In Q-learning, we don't know or even directly estimate $T$ and $R$: we estimate $Q$ directly from experience!
:::

The Q-learning algorithm below adapts this value-iteration idea to the RL scenario, where we do not know the transition function $T$ or
reward function $R$, and instead rely on samples to perform the updates.


```pseudocode
#| html-indent-size: "1.2em"
#| html-comment-delimiter: "//"
#| html-line-number: true
#| html-line-number-punc: ":"
#| html-no-end: false
#| pdf-placement: "htb!"
#| pdf-line-number: true
#| label: Q-Learning

\begin{algorithm}
\begin{algorithmic}[1]
\Procedure{Q-Learning}{$\mathcal{S}, \mathcal{A}, \gamma, \alpha, s_0, \text{max\_iter}$}
  \State $i \gets 0$
  \ForAll{$s \in \mathcal{S},\; a \in \mathcal{A}$}
    \State $Q_{\text{old}}(s, a) \gets 0$
  \EndFor
  \State $s \gets s_0$
  \While{$i < \text{max\_iter}$}
    \State $a \gets \text{select\_action}\bigl(s,\,Q_{\text{old}}(s,a)\bigr)$
    \State $(r, s') \gets \text{execute}(a)$
    \State $Q_{\text{new}}(s, a) \gets (1-\alpha)\,Q_{\text{old}}(s,a)\;+\;\alpha\bigl(r + \gamma \max_{a'}Q_{\text{old}}(s',a')\bigr)$
    \State $s \gets s'$
    \State $i \gets i + 1$
    \State $Q_{\text{old}} \gets Q_{\text{new}}$
  \EndWhile
  \State \Return $Q_{\text{new}}$
\EndProcedure
\end{algorithmic}
\end{algorithm}
```


With the pseudo‑code provided for Q‑Learning, there are a few key things to note. First, we must determine which state to initialize the learning from. In the context of a game, this initial state may be well defined. In the context of a robot navigating an environment, one may consider sampling the initial state at random. In any case, the initial state is necessary to determine the trajectory the agent will experience as it navigates the environment. 

Second, different contexts will influence how we want to choose when to stop iterating through the while loop. Again, in some games there may be a clear terminating state based on the rules of how it is played. On the other hand, a robot may be allowed to explore an environment *ad infinitum*. In such a case, one may consider either setting a fixed number of transitions (as done explicitly in the pseudo‑code) to take; or we may want to stop iterating in the example once the values in the Q‑table are not changing, after the algorithm has been running for a while. 

Finally, a single trajectory through the environment may not be sufficient to adequately explore all state‑action pairs. In these instances, it becomes necessary to run through a number of iterations of the Q‑Learning algorithm, potentially with different choices of initial state $s_0$.

::: {.column-margin}
This notion of running a number of instances of Q‑Learning is often referred to as experiencing multiple *episodes*.
:::

Of course, we would then want to modify Q‑Learning such that the Q table is not reset with each call.

Now, let’s dig into what is happening in Q‑Learning. Here, $\alpha \in (0,1]$ represents the *learning rate*, which needs to decay for convergence purposes, but in practice is often set to a constant. It's also worth mentioning that Q-learning assumes a
discrete state and action space where states and actions take on
discrete values like $1,2,3,\dots$ etc. In contrast, a continuous
state space would allow the state to take values from, say, a
continuous range of numbers; for example, the state could be any real
number in the interval $[1,3]$. Similarly, a continuous action space
would allow the action to be drawn from, e.g., a continuous range of
numbers. There are now many extensions developed based on Q-learning
that can handle continuous state and action spaces (we'll look at one
soon), and therefore the algorithm above is also sometimes referred to
more  specifically as tabular Q-learning.


In the Q-learning update rule

$$
\begin{equation}\label{q_avg}
  Q[s, a] \leftarrow (1-\alpha)Q[s, a]
  + \alpha(r + \gamma \max_{a'}Q[s',a'])
\end{equation}
$${#eq-q-avg}

the term $(r + \gamma \max_{a'}Q[s',a'])$ is often referred to as the one-step look-ahead *target*. The update can be viewed as a
combination of two different iterative processes that we have already
seen: the combination of an old estimate with the target using a
running average with a learning rate $\alpha$, and the
dynamic-programming update of a $Q$ value from value iteration.


@eq-q-avg can also be equivalently rewritten as

$$
\begin{equation}\label{td:q}
  Q[s, a] \leftarrow Q[s, a]
  + \alpha\bigl((r + \gamma \max_{a'} Q[s',a'])-Q[s,a]\bigr),
\end{equation}
$${#eq-td-q}

which allows us to interpret Q-learning in yet another way: we make an update (or correction) based on the temporal difference between the target and the current estimated value $Q[s, a].$



The Q-learning algorithm above includes a procedure called `select_action`, that, given the current state $s$ and current $Q$ function, has to decide
which action to take.  If the $Q$ value is estimated very accurately
and the agent is behaving in the world, then generally we would want
to choose the apparently optimal action $\arg\max_{a \in \mathcal{A}} Q(s,a)$.

But, during learning, the $Q$ value estimates won't be very
good and exploration is important.  However, exploring completely at random is also usually not the best strategy while learning, because it is good to focus your attention on the parts of the state space
that are likely to be visited when executing a good policy (not a bad or random one).

A typical action-selection strategy that attempts to address this *exploration versus exploitation* dilemma is the so-called *$\epsilon$-greedy* strategy:

- with probability $1-\epsilon$, choose $\arg\max_{a \in \mathcal{A}} Q(s,a)$;

- with probability $\epsilon$, choose the action $a \in \mathcal{A}$ uniformly at random.

where the $\epsilon$ probability of choosing a random action helps the agent to explore and try out actions that might not seem so desirable at the moment.

Q-learning has the surprising property that it is *guaranteed* to
converge to the actual optimal $Q$ function under fairly weak
conditions!  Any exploration strategy is okay as long as it tries
every action infinitely often on an infinite run (so that it doesn't
converge prematurely to a bad action choice).

