:::{.callout-caution}
This chapter's HTML notes are in active development. For a more stable version, you can access the PDF notes [here](https://introml.mit.edu/_static/spring25/notes.pdf).
:::

# Reinforcement Learning

*Reinforcement learning* (RL) is a type of machine learning where an agent learns to make decisions by interacting with an environment. Unlike other learning paradigms, RL has several distinctive characteristics:

- The agent interacts directly with an environment, receiving feedback in the form of rewards or penalties
- The agent has agency over what information it seeks to gain from the environment
- The agent updates its decision-making strategy incrementally as it gains more experience

In a reinforcement learning problem, the interaction between the agent and environment follows a specific pattern:

::: {.imagify}
  \begin{tikzpicture}[main node/.style={draw,rounded corners,font=\Large},
      arr/.style={->, thick, shorten <=5pt, shorten >=5pt,}]
    \node[main node] (rl) at (0,1) {Learner};
    \node[main node] (env) at (0,-1) {Environment};
    \draw (env.west) edge[arr, bend left=50, looseness=1.1]
      node[right] {reward} ($ (rl.west) + (0, -0.1)$);
    \draw ($ (env.west) + (0,-0.1)$) edge[arr, bend left=70, looseness=1.5]
      node[left] {state} ($ (rl.west) + (0, 0.1)$);
    \draw (rl.east) edge[arr, bend left=70, looseness=1.5]
      node[right] {action} (env.east);
  \end{tikzpicture}
:::


The interaction cycle proceeds as follows:

1. Agent observes the current *state* $s^{(i)}$
2. Agent selects and executes an *action* $a^{(i)}$
3. Agent receives a *reward* $r^{(i)}$ from the environment
4. Agent observes the new *state* $s^{(i+1)}$
5. Agent selects and executes a new *action* $a^{(i+1)}$
6. Agent receives a new *reward* $r^{(i+1)}$
7. This cycle continues...

Similar to MDP @sec-mdps, in an RL problem, the agent's goal is to learn a *policy* - a mapping from states to actions - that maximizes its expected cumulative reward over time. This policy guides the agent's decision-making process, helping it choose actions that lead to the most favorable outcomes.



## Reinforcement learning algorithms overview {#rl}

A *reinforcement learning (RL) algorithm* is a kind of a policy that depends on the whole history of states, actions, and rewards and selects the next action to take. There are several different ways to measure the quality of an RL algorithm, including:

- Ignoring the $r^{(i)}$ values that it gets *while* learning, but considering how many interactions with the environment are required for it to learn a policy $\pi: \mathcal{S} \rightarrow \mathcal{A}$ that is nearly optimal.

- Maximizing the expected sum of discounted rewards while it is learning.

Most of the focus is on the first criterion (which is called “sample efficiency”), because the second one is very difficult. The first criterion is reasonable when the learning can take place somewhere safe (imagine a robot learning, inside the robot factory, where it can’t hurt itself too badly) or in a simulated environment.

Approaches to reinforcement learning differ significantly according to what kind of hypothesis or model is being learned. Roughly speaking, RL methods can be categorized into model-free methods and model-based methods. The main distinction is that model-based methods explicitly learn the transition and reward models to assist the end-goal of learning a policy; model-free methods do not. We will start our discussion with the model-free methods, and introduce two of the arguably most popular types of algorithms, Q-learning @sec-q_learning and policy gradient @sec-rl_policy_search. We then describe model-based methods @sec-rl_model_based. Finally, we briefly consider "bandit" problems @sec-bandit, which differ from our MDP learning context by having probabilistic rewards.

### Model-free methods

Model-free methods are methods that do not explicitly learn transition and reward models. Depending on what is explicitly being learned, model-free methods are sometimes further categorized into value-based methods (where the goal is to learn/estimate a value function) and policy-based methods (where the goal is to directly learn an optimal policy). It’s important to note that such categorization is approximate and the boundaries are blurry. In fact, current RL research tends to combine the learning of value functions, policies, and transition and reward models all into a complex learning algorithm, in an attempt to combine the strengths of each approach.

### Q-learning {#sec-q_learning}

Q-learning is a frequently used class of RL algorithms that concentrates on learning (estimating) the state-action value function, i.e., the $Q$ function. Specifically, recall the MDP value-iteration update:



$$
\begin{equation}
  Q(s,a) = R(s,a) + \gamma \sum_{s'} T(s,a,s')\max_{a'}Q(s',a')
\end{equation}
$$

::: {.column-margin}
The thing that most students seem to get confused about is when we do value iteration and when we do Q-learning. Value iteration assumes you know $T$ and $R$ and just need to _compute_ $Q$. In Q-learning, we don't know or even directly estimate $T$ and $R$: we estimate $Q$ directly from experience!
:::

The Q-learning algorithm below adapts this value-iteration idea to the RL scenario, where we do not know the transition function $T$ or
reward function $R$, and instead rely on samples to perform the updates.

