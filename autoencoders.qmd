:::{.callout-note}
This page contains all content from the legacy [PDF notes](https://introml.mit.edu/_static/spring25/notes.pdf); autoencoders chapter.

As we phase out the PDF, this page may receive updates not reflected in the static PDF.
:::

# Autoencoders {#sec-autoencoders}

In previous chapters, we have largely focused on classification and regression problems, where we use supervised learning with training samples that have both features/inputs and corresponding outputs or labels, to learn hypotheses or models that can then be used to predict labels for new data.

In contrast to supervised learning paradigm, we can also have an unsupervised learning setting, where we only have features but no corresponding outputs or labels for our dataset. On natural question aries then: if there are no labels, what are we learning?

One canonical example of unsupervised learning is clustering, which is discussed in @sec-clustering. In clustering, the goal is to develop algorithms that can reason about "similarity" among data points's features, and group the data points into clusters.

*Autoencoders* are another family of unsupervised learning algorithms, in this case seeking to obtain insights about our data by learning compressed versions of the original data, or, in other words, by finding a good lower-dimensional feature representations of the same data set. Such insights might help us to discover and characterize underlying factors of variation in data, which can aid in scientific discovery; to compress data for efficient storage or communication; or to pre-process our data prior to supervised learning, perhaps to reduce the amount of data that is needed to learn a good classifier or regressor.

## Autoencoder structure

Assume that we have input data $\mathcal{D} = \{x^{(1)}, \ldots,
  x^{(n)} \}$, where $x^{(i)}\in \mathbb{R}^d$. We seek to learn an autoencoder that will output a new dataset $\mathcal{D}_{out} =
  \{a^{(1)}, \ldots, a^{(n)}\}$, where $a^{(i)}\in \mathbb{R}^k$ with $k
  < d$. We can think about $a^{(i)}$ as the new *representation* of data point $x^{(i)}$. For example, in @fig-illustration we show the learned representations of a dataset of MNIST digits with $k=2$. We see, after inspecting the individual data points, that unsupervised learning has found a compressed (or *latent*) representation where images of the same digit are close to each other, potentially greatly aiding subsequent clustering or classification tasks.

![Compression of digits dataset into two dimensions. The input $x^{(i)}$, an image of a handwritten digit, is shown at the new low-dimensional representation $(a_1,a_2)$.](figures/autoencoder_mnist.png){#fig-illustration width="50%"}

Formally, an autoencoder consists of two functions, a vector-valued *encoder* $g : \mathbb{R}^d \rightarrow \mathbb{R}^k$ that deterministically maps the data to the representation space $a \in
  \mathbb{R}^k$, and a *decoder* $h : \mathbb{R}^k \rightarrow
  \mathbb{R}^d$ that maps the representation space back into the original data space.

In general, the encoder and decoder functions might be any functions appropriate to the domain. Here, we are particularly interested in neural network embodiments of encoders and decoders. The basic architecture of one such autoencoder, consisting of only a single layer neural network in each of the encoder and decoder, is shown in @fig-autoencoder; note that bias terms $W^1_0$ and $W^2_0$ into the summation nodes exist, but are omitted for clarity in the figure. In this example, the original $d$-dimensional input is compressed into $k=3$ dimensions via the encoder $g(x; W^1, W^1_0)=f_1(W{^1}^T x + W^1_0)$ with $W^1 \in
  \mathbb{R}^{d \times k}$ and $W^1_0 \in \mathbb{R}^k$, and where the non-linearity $f_1$ is applied to each dimension of the vector. To recover (an approximation to) the original instance, we then apply the decoder $h(a; W^2, W^2_0) = f_2(W{^2}^T
  a + W^2_0)$, where $f_2$ denotes a different non-linearity (activation function). In general, both the decoder and the encoder could involve multiple layers, as opposed to the single layer shown here. Learning seeks parameters $W^1, W^1_0$ and $W^2, W^2_0$ such that the reconstructed instances, $h(g(x^{(i)}; W^{1}, W^1_0); W^{2}, W^2_0)$, are close to the original input $x^{(i)}$.

![Autoencoder structure, showing the encoder (left half, light green), and the decoder (right half, light blue), encoding inputs $x$ to the representation $a$, and decoding the representation to produce $\tilde{x}$, the reconstruction. In this specific example, the representation ($a_1$, $a_2$, $a_3$) only has three dimensions.[]{#fig:autoencoder label="fig:autoencoder"}](figures/autoencoder.png){#fig-autoencoder width=".8"}

## Autoencoder Learning

We learn the weights in an autoencoder using the same tools that we previously used for supervised learning, namely (stochastic) gradient descent of a multi-layer neural network to minimize a loss function. All that remains is to specify the loss function $\mathcal{L}(\tilde{x}, x)$, which tells us how to measure the discrepancy between the reconstruction $\tilde{x} = h(g(x; W^{1}, W^1_0); W^{2}, W^2_0)$ and the original input $x$. For example, for continuous-valued $x$ it might make sense to use squared loss, i.e., $\mathcal{L}_{SE}(\tilde{x}, x) = \sum_{j=1}^{d} (x_j - \tilde{x}_j)^2$. 

:::{.column-margin}
Alternatively, you could think of this as *multi-task learning*, where the goal is to predict each dimension of $x$. One can mix-and-match loss functions as appropriate for each dimension's data type.
:::

Learning then seeks to optimize the parameters of $h$ and $g$ so as to minimize the reconstruction error, measured according to this loss function: $$\min_{W^{1}, W^1_0, W^{2}, W^2_0} \sum_{i=1}^n \mathcal{L}_{SE}\left(h(g(x^{(i)}; W^{1}, W^1_0); W^{2}, W^2_0), x^{(i)}\right)$$

## Evaluating an autoencoder

What makes a good learned representation in an autoencoder? Notice that, without further constraints, it is always possible to perfectly reconstruct the input. For example, we could let $k=d$ and $h$ and $g$ be the identity functions. In this case, we would not obtain any compression of the data.

To learn something useful, we must create a *bottleneck* by making $k$ to be smaller (often much smaller) than $d$. This forces the learning algorithm to seek transformations that describe the original data using as simple a description as possible. Thinking back to the digits dataset, for example, an example of a compressed representation might be the digit label (i.e., 0--9), rotation, and stroke thickness. Of course, there is no guarantee that the learning algorithm will discover precisely this representation. After learning, we can inspect the learned representations, such as by artificially increasing or decreasing one of the dimensions (e.g., $a_1$) and seeing how it affects the output $h(a)$, to try to better understand what it has learned.

As with clustering, autoencoders can be a preliminary step toward building other models, such as a regressor or classifier. For example, once a good encoder has been learned, the decoder might be replaced with another neural network that is then trained with supervised learning (perhaps using a smaller dataset that does include labels).

## Linear encoders and decoders

We close by mentioning that even linear encoders and decoders can be very powerful. In this case, rather than minimizing the above objective with gradient descent, a technique called *principal components analysis* (PCA) can be used to obtain a closed-form solution to the optimization problem using a singular value decomposition (SVD). Just as a multilayer neural network with nonlinear activations for regression (learned by gradient descent) can be thought of as a nonlinear generalization of a linear regressor (fit by matrix algebraic operations), the neural network based autoencoders discussed above (and learned with gradient descent) can be thought of as a generalization of linear PCA (as solved with matrix algebra by SVD).

## Advanced encoders and decoders

Advanced neural networks that build on the encoder-decoder conceptual decomposition have become increasingly powerful in recent years. One family of applications are *generative* networks, where new outputs that are “similar to” but different from any existing training sample are desired. In *variational autoencoders* the compressed representation encompasses information about the probability distribution of training samples, e.g., learning both mean and standard deviation variables in the bottleneck layer or latent representation. Then, new outputs can be generated by random sampling based on the latent representation variables and feeding those samples into the decoder.

*Transformer* neural networks, which we will explore later, use multiple encoder and decoder blocks, together with a self-attention mechanism to make predictions about likely next elements in a sequence. Such architectures have been especially impactful in natural language processing (NLP), where the goal is to model and generate sequences of words, sentences, or even documents.

At the core of both autoencoders and transformers is the idea of *representation*. In an autoencoder, we compress and reconstruct data using a learned representation of the input. In NLP, we seek to represent language data—words, phrases, and sentences—in such a way that their meanings and relationships can be captured numerically. That brings us to a crucial next step: vector embeddings. 

## Vector Embeddings

Before we can fully understand the attention mechanism used in Transformers, we need to introduce a new way of representing language data in neural networks.

In NLP, words are often represented as *vectors*, commonly referred to as *word embeddings*. These embeddings are designed so that their numerical similarity corresponds to semantic similarity. For example, words like dog and cat—which are conceptually related—should be represented by vectors that are closer to each other in some high-dimensional space than unrelated pairs like cat and table.

A commonly used metric to compare the similarity between two embeddings is *inner product similarity*:

$$\begin{aligned}
  u^T v = u \cdot v
\end{aligned}$$ The result of the inner product can range from negative infinity to positive infinity. When two vectors are very similar and point in the same direction, their inner product is highly positive. If they point in completely opposite directions, the inner product becomes highly negative. When the vectors are perpendicular (i.e., have no directional alignment), the inner product is zero, indicating no similarity. In general, larger positive values suggest a stronger semantic connection between the words.

Importantly, as we’ll see when we discuss attention-based models, the same word may have different embeddings depending on context. That is, the word “bank” in river bank and bank account may be represented by entirely different vectors, shaped by the surrounding words. In fact, it's now typical for each individual occurrence of a word to have its own unique embedding, even within the same document. For example, in a story that mentions the word “dog” a dozen times, each instance may be encoded slightly differently, depending on its role and context within the sentence and narrative.

## Embedding Space

One of the most compelling properties of word embeddings is that they do more than just capture pairwise similarity—they can encode meaningful relationships between words as well.

While word embeddings—and various approaches to create them—have existed for decades, the first approach that produced astonishingly effective embeddings was *word2vec*, introduced in 2012. This revolutionary method was the first highly successful application of deep learning to NLP, and it enabled nearly all the subsequent progress in the field, including the development of Transformers.

The details of *word2vec* are beyond the scope of this course, but it's worth noting that it produced word embeddings so useful that many relationships between the vectors corresponded with real-world semantic relatedness.

For example, when using *Euclidean distance* as a distance metric between two vectors, word2vec produced word embeddings with properties such as (where $v_{\tt word}$ is the vector for ${\tt word}$):

$$\begin{aligned}
  v_{\tt paris} - v_{\tt france} + v_{\tt italy} \approx v_{\tt rome}
\end{aligned}$$

This corresponds with the real-world property that Paris is to France what Rome is to Italy. This incredible finding existed not only for geographic words but all sorts of real-world concepts in the vocabulary (e.g., king - man + woman ≈ queen). 

That said, the exact coordinates of a given word vector are arbitrary; what matters is the relative arrangement of all the vectors in the embedding space. In practice, an embedding is judged to be “good” if it is useful for a downstream task—such as predicting the next word in a sequence, classifying text, or translating languages.

For example, an embedding may be considered good if it accurately captures the conditional probability for a given word to appear next in a sequence of words. You probably have a good idea of what words might typically fill in the blank at the end of this sentence:

> After the rain, the grass was ____

Or a model could be built that tries to correctly predict words in the middle of sentences:

> The child fell __ __ during the long car ride

The model can be built by minimizing a loss function that penalizes incorrect word guesses, and rewards correct ones. This is done by training a model on a very large corpus of written material, such as all of Wikipedia, or even all the accessible digitized written materials produced by humans.

However, it’s important to note that these word2vec embeddings were not context-aware. Each word had a unique fixed vector, regardless of where or how it appeared. So, the word *“bank”* would be represented the same way whether it appeared in *“river bank”* or *“bank account”*, even though the meanings are clearly different.

We’ll return to the concept of embedding space soon—but with a powerful twist. In Transformer models, each word occurrence gets its own context-sensitive embedding that shifts dynamically depending on surrounding words. This opens the door to much more nuanced understanding of language.
