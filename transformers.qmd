:::{.callout-caution}
We are actively overhauling the Transformers chapter from the legacy [PDF notes](https://introml.mit.edu/_static/spring25/notes.pdf) to enhance clarity and presentation—stay tuned for updates!
:::


# Transformers {#sec-transformers}

Transformers are a very recent family of architectures that have revolutionized fields like natural language processing (NLP), image processing, and multi-modal generative AI.

Transformers were originally introduced in the field of NLP in 2017, as an approach to process and understand human language. 

:::{.column-margin}
Human language is inherently sequential in nature (e.g., characters form words, words form sentences, and sentences form paragraphs and documents). Prior to the advent of the transformers architecture, recurrent neural networks (RNNs) briefly dominated the field for their ability to process sequential information. However, RNNs, like many other architectures, processed sequential information in an iterative/sequential fashion, whereby each item of a sequence was individually processed one after another. Transformers offer many advantages over RNNs, including their ability to process all items in a sequence in a *parallel* fashion (as do CNNs).
:::

Like CNNs, transformers factorize the signal processing problem into stages that involve independent and identically processed chunks. Transformers have many intricate parts, we will focus on the most crucial new type of layer that mix information across the chunks, called *attention layers,* which is the layer that enabled the the full transformers pipeline can model dependencies between the chunks. To help make Transformers more digestible, in this chapter, we will first succinctly motivate and describe them in an overview @sec-transformers-overview. Then, we will dive into the details following the flow of data -- first describing how to represent inputs @sec-embeddings, and then describe the attention mechanism @sec-qkv, and finally we then assemble all these ideas together to arrive at the full transformer architecture in @sec-transformers-arch.

## Transformers Overview {#sec-transformers-overview}

## Embedding and Representations {#sec-embeddings}

## Query, Key, Value, and Attention Output {#sec-qkv}

Attention is a strategy for processing global information efficiently, focusing just on the parts of the signal that are most salient to the task at hand.

:::{.column-margin}
What we present below is the so-called ``dot-product attention'' mechanism; there can be other variants that involve more complex attention functions.
:::

It might help our understanding of the "attention" mechanism to think about a dictionary look-up scenario. Consider a dictionary with keys $k$ mapping to some values $v$. For example, let $k$ be the name of some foods, such as `pizza`, `apple`, `sandwich`, `donut`, `chili`, `burrito`, `sushi`, `hamburger`, $\ldots$. The corresponding values $v$ may be information about the food, such as where it is available, how much it costs, or what its ingredients are.

Suppose that instead of looking up food information by a specific name, we want to query by cuisine, e.g., "`mexican`" foods. Clearly, we cannot simply look for the word "`mexican`" among the dictionary keys, since that word is not a food. What does work is to utilize again the idea of finding "similarity" between vector embeddings of the query and the keys. Roughly, we first embed both the query "`mexican`" and the dictionary entries (such as `pizza`, `apple`, `sandwich`) into a same embedding space, where semantically related words are close to each other. This enables us to retrieve relevant entries based on semantic similarity rather than exact string matching.

Formally, we follow these steps: First, embed the word we are interested in ("`mexican`" in our example) into a so-called query vector, denoted simply as $q \in \mathbb{R}^{d_k}$ where $d_k$ is the embedding dimension.

Next, suppose our given dictionary has $n$ number of entries/keys, we embed each one of these into a so-called key vector. In particular, for the $j$th entry in the dictionary, we produce a $k_j \in \mathbb{R}^{d_k}$ key vector, where $j=1,2,3, \dots, n$.

We can then apply a softmax (@sec-classification, @sec-neural_networks) to the inner-product between the key and query:

$$
\alpha = \text{softmax}\left([q^T k_1; q^T k_2; q^T k_3; \dots, q^T k_n]\right) \in \mathbb{R}^{n}
$$

The raw inner-product $q^T k_j$ represents how similar the query is to the $j$th dictionary entry. However, on their own, these scores are not directly interpretable—they can be any real number, positive or negative, and don’t have any clear scale. That’s why we apply the softmax function, which transforms the raw scores into a normalized vector (with all entries between 0 and 1, and sum up to 1). 

We will refer to $\alpha$ as "softmax'd attention scores", since the $j$th element in $\alpha$ (denoted as $\alpha_j$) states how much attention should be given to the key $k_j$ for a given query $q$. 

:::{.column-margin}
In the literature, oftentimes $\alpha$ will be referred to as "attention weights". The wording "weights" is quite misleadning -- because we do not get to directly learn $\alpha$ as we do with other network parameters. As such, we will more verbosely but explicitly refer to $\alpha$ as "softmax'd attention score"
:::



With $\alpha$, we can calculate the "attention output" 

$$
\begin{aligned}
  z = \sum_{j=1}^n \alpha_j \ v_j
\end{aligned}
$$





The meaning of this weighted average value may be ambiguous when the values are 
just words. However, the attention output really becomes meaningful when the values are projected in some semantic embedding space as the query and keys (and such projections are typically done in transformers via learned embedding weights). In the next section, we will formally introduce the self-attention operation used in the Transformer architecture.

## Self-attention Layer {#sec-self-attention}

Self-attention is an attention mechanism where the keys, values, and queries are all generated from the same input.

At a very high level, typical transformer with self-attention layers maps $\mathbb{R}^{n\times d} \longrightarrow \mathbb{R}^{n\times d}$. In particular, the transformer takes in $n$ tokens, each having feature dimension $d$. Thus, all tokens can be collectively written as $X\in \mathbb{R}^{n\times d}$, where the $i$-th row of $X$ stores the $i$-th token, denoted as $x_i\in\mathbb{R}^{1\times d}$. For each token $x^i$, self-attention computes (via learned projection, to be discussed in @sec-learnedembedding), a query $q_i \in \mathbb{R}^{d_q\times 1}$, key $k_{i} \in \mathbb{R}^{d_k\times 1}$, and value $v_{i} \in \mathbb{R}^{d_v\times 1}$, and overall, we will have $n$ queries, $n$ keys, and $n$ values. In practice, $d_q=d_k=d_v$ and we often denote all three embedding dimension via a unified $d_k$. 

Let's begin by considering what self-attention does for a specific query $q_i$. Concretely, self-attention first calculates the attention weights:

$$\alpha_i = \text{softmax}\left(
  [q_i^T k_1; q_i^T k_2; q_i^T k_3; \dots, q_i^T k_n] / \sqrt{d_k}
  \right)\in\mathbb{R}^{1\times n}$$

In this equation, the normalization by $\sqrt{d_k}$ is done to reduce the magnitude of the dot product, which would otherwise grow undesirably large with increasing $d_k,$ making it difficult for (overall) training. Then the attention output is calculated as a weighted sum:

$$
{z}_i = \sum_{j=1}^n  \alpha_{ij} v_{j}\in\mathbb{R}^{d_k \times 1}
$$where $\alpha_{ij}$ is the $j$th element in $\alpha_i$.

Note that the above procedure should sound familiar since it is almost the same depicted in @sec-qkv, and perhaps the only difference is that we have deliberately omitted the normalization factor (i.e., divided by $\sqrt{d_k}$) in @sec-qkv for introduction purpose. 

### Self-attention in Matrix Form {#sec-matrix-self-attn}

<!-- TODO: Add Q and K row stack equation-->

<!-- TODO: add a note that often times you see the A = softmax (QK^T/sqrt{}), it's a slight abuse of notation and should be more explicitly softmax_{row} -->
Importantly, self-attention is independent for every query $q_i$ which means it essentially performs the same procedure independently for every query. By stacking these computations together, we can express the full attention matrix $A \in \mathbb{R}^{n \times n}$ as follows:

$$\begin{aligned}
A = \begin{bmatrix}
    \text{softmax}\left( \begin{bmatrix}
                           q_1^\top k_1 & q_1^\top k_2 & \cdots & q_1^\top k_{n}
                         \end{bmatrix} / \sqrt{d_k} \right) \\
    \text{softmax}\left( \begin{bmatrix}
                           q_2^\top k_1 & q_2^\top k_2 & \cdots & q_2^\top k_{n}
                         \end{bmatrix} / \sqrt{d_k} \right) \\
    \vdots &                                                                   \\
    \text{softmax}\left( \begin{bmatrix}
                           q_{n}^\top k_1 & q_{n}^\top k_2 & \cdots & q_{n}^\top k_{n}
                         \end{bmatrix} / \sqrt{d_k} \right)
  \end{bmatrix}
\end{aligned}
$${#eq-self_softmax}


The $i$th row $A$ of this matrix corresponds to the attention weights computed for query $q_i$ over all keys (i.e., $\alpha_i$). The full output of the self-attention layer can then be written compactly as:

$$
Z=A V
$$
where $V\in \mathbb{R}^{n \times d_k}$ is the matrix of value vectors stacked row-wise, and $Z \in \mathbb{R}^{n \times d_k}$ is the output, whose $i$th row corresponds to the attention output for the $i$th query (i.e., $z_i$).



### Masked Self-attention {#sec-masked-attention}
More generally, a *mask* may be applied to limit which tokens are used in the attention computation. For example, one common mask limits the attention computation to tokens that occur previously in time to the one being used for the query. This prevents the attention mechanism from "looking ahead" in scenarios where the transformer is being used to generate one token at a time. This causal masking is done by introducing a mask matrix $M \in \mathbb{R}^{n \times n}$ that restricts attention to only current and previous positions. A typical causal mask is a lower-triangular matrix:

$$
M = 
\begin{bmatrix}
  0 & -\infty & -\infty & \cdots & -\infty \\
  0 & 0       & -\infty & \cdots & -\infty \\
  0 & 0       & 0       & \cdots & -\infty \\
  \vdots & \vdots & \vdots & \ddots & \vdots \\
  0 & 0       & 0       & \cdots & 0
\end{bmatrix}
$$

and we now have the masked attention matrix:

$$\begin{aligned}
A = \text{softmax}\left(
\frac{1}{\sqrt{d_k}}
\begin{bmatrix}
q_1^\top k_1 & q_1^\top k_2 & \cdots & q_1^\top k_n \\
q_2^\top k_1 & q_2^\top k_2 & \cdots & q_2^\top k_n \\
\vdots       & \vdots       & \ddots & \vdots       \\
q_n^\top k_1 & q_n^\top k_2 & \cdots & q_n^\top k_n
\end{bmatrix}
+ M \right)\end{aligned}
$$ The softmax is performed to each row independently. The attention output is still $Y= A V$. Essentially, the lower-triangular property of $M$ ensures that the self-attention operation for the $j$-th query only considers tokens $0,1,...,j$. Note that we should apply the masking before performing softmax, so that the attention matrix can be properly normalized (i.e., each row sum to 1).


Each self-attention stage is trained to have key, value, and query embeddings that lead it to pay specific attention to some particular feature of the input. We generally want to pay attention to many different kinds of features in the input; for example, in translation one feature might be be the verbs, and another might be objects or subjects. A transformer utilizes multiple instances of self-attention, each known as an "attention head," to allow combinations of attention paid to many different features.

## Transformers Architecture Details {#sec-transformers-arch-details}

### Multi-headed Attention

### Positional Embeddings 

A transformer is the composition of a number of transformer blocks, each of which has multiple attention heads. At a very high-level, the goal of a transformer block is to output a really rich, useful representation for each input token, all for the sake of being high-performant for whatever task the model is trained to learn.


### Variations and training

Many variants on this transformer structure exist. For example, the $\text{LayerNorm}$ may be moved to other stages of the neural network. Or a more sophisticated attention function may be employed instead of the simple dot product used in @eq-xfm_softmax. Transformers may also be used in pairs, for example, one to process the input and a separate one to generate the output given the transformed input. Self-attention may also be replaced with cross-attention, where some input data are used to generate queries and other input data generate keys and values. Positional encoding and masking are also common, though they are left implicit in the above equations for simplicity.

How are transformers trained? The number of parameters in $\theta$ can be very large; modern transformer models like GPT4 have tens of billions of parameters or more. A great deal of data is thus necessary to train such models, else the models may simply overfit small datasets.

Training large transformer models is thus generally done in two stages. A first "pre-training" stage employs a very large dataset to train the model to extract patterns. This is done with unsupervised (or self-supervised) learning and unlabelled data. For example, the well-known BERT model was pre-trained using sentences with words masked. The model was trained to predict the masked words. BERT was also trained on sequences of sentences, where the model was trained to predict whether two sentences are likely to be contextually close together or not. The pre-training stage is generally very expensive.

The second "fine-tuning" stage trains the model for a specific task, such as classification or question answering. This training stage can be relatively inexpensive, but it generally requires labeled data.

